{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from unixcoder import UniXcoder\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = UniXcoder(\"microsoft/unixcoder-base\")\n",
    "model.to(device)\n",
    "\n",
    "def unixcoder_sim(text1,text2):\n",
    "# Encode maximum function\n",
    "    tokens_ids = model.tokenize([text1],max_length=512,mode=\"<encoder-only>\")\n",
    "    source_ids = torch.tensor(tokens_ids).to(device)\n",
    "    tokens_embeddings,max_func_embedding = model(source_ids)\n",
    "    \n",
    "    # Encode minimum function\n",
    "    tokens_ids = model.tokenize([text2],max_length=512,mode=\"<encoder-only>\")\n",
    "    source_ids = torch.tensor(tokens_ids).to(device)\n",
    "    tokens_embeddings,min_func_embedding = model(source_ids)\n",
    "\n",
    "    norm_max_func_embedding = torch.nn.functional.normalize(max_func_embedding, p=2, dim=1)\n",
    "    norm_min_func_embedding = torch.nn.functional.normalize(min_func_embedding, p=2, dim=1)\n",
    "    return torch.einsum(\"ac,bc->ab\",norm_min_func_embedding,norm_max_func_embedding)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "import numpy as np\n",
    "import os\n",
    "import javalang\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "from rouge import Rouge \n",
    "import re\n",
    "import numpy as np\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def level_acc(classification_pred, classification_label) -> float:\n",
    "    level_map = {'trace':0., 'debug':1., 'info':2., 'warn':3., 'error':4.}\n",
    "    new_pred = []\n",
    "    new_label = []\n",
    "    length = len(classification_pred)\n",
    "    for idx in range(length):\n",
    "        predict = classification_pred[idx]\n",
    "        label = classification_label[idx]\n",
    "        if predict in level_map.keys() and label in level_map.keys():\n",
    "            pred_sum = level_map[predict]\n",
    "            label_sum = level_map[label]\n",
    "            new_pred.append(pred_sum)\n",
    "            new_label.append(label_sum)\n",
    "    matches = sum(x == y for x, y in zip(new_pred, new_label))\n",
    "    total_elements = len(new_pred)\n",
    "    accuracy = matches / total_elements\n",
    "    return accuracy\n",
    "\n",
    "def query_level(level: float) -> str:\n",
    "    if level == 1.:\n",
    "        return 'trace'\n",
    "    elif level == 2.:\n",
    "        return 'debug'\n",
    "    elif level == 3.:\n",
    "        return 'info'\n",
    "    elif level == 4.:\n",
    "        return 'warn'\n",
    "    elif level == 5.:\n",
    "        return 'error'\n",
    "    else:\n",
    "        return ''\n",
    "        \n",
    "def aod(classification_pred, classification_label) -> float:\n",
    "    level_map = {'trace':1., 'debug':2., 'info':3., 'warn':4., 'error':5.}\n",
    "    max_distance = {'trace':4., 'debug':3., 'info':2., 'warn':3., 'error':4.}\n",
    "\n",
    "    distance_sum = 0.\n",
    "    noise = 0.\n",
    "    length = len(classification_pred)\n",
    "    \n",
    "    for idx in range(length):\n",
    "        try:\n",
    "            predict = classification_pred[idx]\n",
    "            label = classification_label[idx]\n",
    "            pred_sum = level_map[predict]\n",
    "            label_sum = level_map[label]\n",
    "            level = query_level(label_sum)\n",
    "            _distance = abs(label_sum - pred_sum)\n",
    "            distance_sum = distance_sum + (1 - _distance / max_distance[level])\n",
    "        except Exception as e:\n",
    "            noise = noise+1\n",
    "    aod = distance_sum / (length-noise)    \n",
    "    return aod\n",
    "\n",
    "def precision_recall_f1(gt, pd):\n",
    "    intersection = len(gt.intersection(pd))\n",
    "    \n",
    "    if len(gt) == 0 and len(pd) == 0:\n",
    "        precision = 1\n",
    "        recall = 1\n",
    "    elif len(gt) == 0:\n",
    "        precision = intersection / len(pd)\n",
    "        recall = 1\n",
    "    elif len(pd) == 0:\n",
    "        precision = 1\n",
    "        recall = intersection / len(gt)\n",
    "    else:\n",
    "        precision = intersection / len(pd)\n",
    "        recall = intersection / len(gt)\n",
    "    \n",
    "    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "    return precision, recall, f1\n",
    "\n",
    "\n",
    "\n",
    "def tokenize_java_code(code):\n",
    "    tokens = list(javalang.tokenizer.tokenize(code))\n",
    "    return tokens\n",
    "\n",
    "def is_java_string(token):\n",
    "    return isinstance(token, javalang.tokenizer.String)\n",
    "\n",
    "def get_list4bleu(java_code):\n",
    "    tokenized_java_code = tokenize_java_code(java_code)\n",
    "    my_list = []\n",
    "    for token in tokenized_java_code:\n",
    "        if is_java_string(token):\n",
    "            my_list.extend(token.value.split())\n",
    "        else:\n",
    "            my_list.append(token.value)\n",
    "    return my_list\n",
    "\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "\n",
    "def calculate_bleu_scores(reference, candidate):\n",
    "    reference = get_list4bleu(reference)\n",
    "    candidate = get_list4bleu(candidate)\n",
    "    smooth = SmoothingFunction().method1\n",
    "\n",
    "    bleu1 = sentence_bleu([reference], candidate, weights=(1, 0, 0, 0), smoothing_function=smooth)\n",
    "    bleu2 = sentence_bleu([reference], candidate, weights=(0.5, 0.5, 0, 0), smoothing_function=smooth)\n",
    "    bleu3 = sentence_bleu([reference], candidate, weights=(0.33, 0.33, 0.33, 0), smoothing_function=smooth)\n",
    "    bleu4 = sentence_bleu([reference], candidate, weights=(0.25, 0.25, 0.25, 0.25), smoothing_function=smooth)\n",
    "\n",
    "    return bleu1, bleu2, bleu3, bleu4\n",
    "\n",
    "\n",
    "def extract_quoted_strings(s):\n",
    "    quoted_strings = re.findall(r'\"([^\"]*)\"', s)\n",
    "    \" \".join(quoted_strings)\n",
    "    remaining = re.sub(r'\"[^\"]*\"', '', s)\n",
    "    char_to_remove = ['+', ',']\n",
    "    for char in char_to_remove:\n",
    "        remaining = remaining.replace(char, '')\n",
    "    var_list_origin = remaining.split(' ')\n",
    "    var_list = [item for item in var_list_origin if (not item == ' ')]\n",
    "    var_list = [item for item in var_list if item]\n",
    "    return quoted_strings, var_list\n",
    "\n",
    "def extract_outer_brackets(s):\n",
    "    stack = []\n",
    "    result = []\n",
    "\n",
    "    for m in re.finditer(r\"[()]\", s):\n",
    "        char, pos = m.group(0), m.start(0)\n",
    "        if char == \"(\":\n",
    "            stack.append(pos)\n",
    "        elif char == \")\":\n",
    "            if len(stack) == 1:\n",
    "                result.append(s[stack.pop() + 1:pos])\n",
    "            else:\n",
    "                stack.pop()\n",
    "    return result\n",
    "\n",
    "def average_precision_recall_f1(gt_list, pd_list):\n",
    "    total_precision, total_recall, total_f1 = 0, 0, 0\n",
    "    n = len(gt_list)\n",
    "\n",
    "    for gt, pd in zip(gt_list, pd_list):\n",
    "        precision, recall, f1 = precision_recall_f1(gt, pd)\n",
    "        total_precision += precision\n",
    "        total_recall += recall\n",
    "        total_f1 += f1\n",
    "\n",
    "    avg_precision = total_precision / n\n",
    "    avg_recall = total_recall / n\n",
    "    avg_f1 = total_f1 / n\n",
    "\n",
    "    return avg_precision, avg_recall, avg_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#code_whisperer\n",
    "#tabnine\n",
    "#codeGeex\n",
    "\n",
    "path = \"./codeGeex_function_result/\"\n",
    "ground_truth_folder = './LogBench-O_prefix_1point/'\n",
    "def extract_numbers(s):\n",
    "    return re.findall(r'\\d+', s)\n",
    "\n",
    "results = {}\n",
    "\n",
    "def parse_directory(dir_path):\n",
    "    for filename in os.listdir(dir_path):\n",
    "        file_path = os.path.join(dir_path, filename)\n",
    "        if os.path.isfile(file_path) and file_path.endswith('.java'):\n",
    "            ground_truth_path = ground_truth_folder+file_path.split('/')[-1][:-5]+'_config.txt'\n",
    "            with open(ground_truth_path) as f:\n",
    "                    lines = f.readlines()\n",
    "                    if len(lines) >= 1:\n",
    "                        line_number = int(extract_numbers(lines[0].strip(' ')[:-1])[0])\n",
    "            with open(file_path) as f:\n",
    "                lines = f.readlines()\n",
    "                if len(lines) >= 4:\n",
    "                    results[file_path.split(\"/\")[-1]] = lines[line_number-1].strip(' ')[:-2]\n",
    "                else:\n",
    "                    pass\n",
    "        elif os.path.isdir(file_path):\n",
    "            parse_directory(file_path)\n",
    "parse_directory(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#incoder\n",
    "import os\n",
    "path = \"./Incoder_function_level_what2log_1point_infill_result/\"\n",
    "\n",
    "results = {}\n",
    "\n",
    "def parse_directory(dir_path):\n",
    "    for filename in os.listdir(dir_path):\n",
    "        file_path = os.path.join(dir_path, filename)\n",
    "        if os.path.isfile(file_path) and file_path.endswith('.java'):\n",
    "            with open(file_path) as f:\n",
    "                lines = f.readlines()\n",
    "                if len(lines)>0:\n",
    "                    results[file_path.split(\"/\")[-1]] = \"log.\"+lines[0].strip(' ')[:-1]\n",
    "        elif os.path.isdir(file_path):\n",
    "            parse_directory(file_path)\n",
    "parse_directory(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#copilot\n",
    "path = \"./Copilot_Function-copilot_result/\"\n",
    "\n",
    "results = {}\n",
    "\n",
    "def parse_directory(dir_path):\n",
    "    for filename in os.listdir(dir_path):\n",
    "        file_path = os.path.join(dir_path, filename)\n",
    "        if os.path.isfile(file_path) and file_path.endswith('.txt'):\n",
    "            with open(file_path) as f:\n",
    "                lines = f.readlines()\n",
    "                if len(lines) > 4:\n",
    "                    results[file_path.split(\"/\")[-1][:-4]] = lines[3].strip(' ')[:-1]\n",
    "                else:\n",
    "                    continue\n",
    "        elif os.path.isdir(file_path):\n",
    "            parse_directory(file_path)\n",
    "parse_directory(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#llama\n",
    "path = \"./results_llamas/llama2/\"\n",
    "\n",
    "results = {}\n",
    "\n",
    "def parse_directory(dir_path):\n",
    "    for filename in os.listdir(dir_path):\n",
    "        file_path = os.path.join(dir_path, filename)\n",
    "        if os.path.isfile(file_path) and file_path.endswith('.java'):\n",
    "            with open(file_path) as f:\n",
    "                lines = f.readlines()\n",
    "                if len(lines) == 1:\n",
    "                    results[file_path.split(\"/\")[-1]] = lines[0].strip(' ').split('statement: ')[-1]\n",
    "                elif len(lines) == 3:\n",
    "                    results[file_path.split(\"/\")[-1]] = lines[2].strip(' ').split('statement: ')[-1]\n",
    "                else:\n",
    "                    continue\n",
    "        elif os.path.isdir(file_path):\n",
    "            parse_directory(file_path)\n",
    "parse_directory(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#StarCoder\n",
    "path = \"./StarCoder_LogBench-O_prefix_1point/\"\n",
    "\n",
    "results = {}\n",
    "\n",
    "def parse_directory(dir_path):\n",
    "    for filename in os.listdir(dir_path):\n",
    "        file_path = os.path.join(dir_path, filename)\n",
    "        if os.path.isfile(file_path) and file_path.endswith('.java'):\n",
    "            with open(file_path) as f:\n",
    "                lines = f.readlines()\n",
    "                for line in lines:\n",
    "                    if '<fim_middle>' in line:\n",
    "                        if '<|endoftext|>' in line:\n",
    "                            results[file_path.split(\"/\")[-1]] ='log.' + line.strip().split('<fim_middle>')[-1].split(';')[0]\n",
    "                        else:\n",
    "                            results[file_path.split(\"/\")[-1]] ='log.' + line.strip().split('<fim_middle>')[-1]\n",
    "        elif os.path.isdir(file_path):\n",
    "            parse_directory(file_path)\n",
    "parse_directory(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3760"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_truth_folder = './LogBench-O_prefix_1point/'\n",
    "generated_level = []\n",
    "real_level = []\n",
    "generated_var = []\n",
    "real_var = []\n",
    "generated_string_list = []\n",
    "truth_string_list = []\n",
    "bleu_score_list = []\n",
    "rouge_list = []\n",
    "ulqi_list = []\n",
    "emb_list = []\n",
    "for key, value in results.items():\n",
    "    temp_dict = dict()\n",
    "    try:\n",
    "        string = extract_outer_brackets(value)[0]\n",
    "        quoted_strings, remaining = extract_quoted_strings(string)\n",
    "        quoted_strings = ' '.join(quoted_strings)\n",
    "        ground_truth_path = ground_truth_folder+key[:-5]+'_config.txt'\n",
    "        with open(ground_truth_path) as f:\n",
    "                lines = f.readlines()\n",
    "                if len(lines) >= 2:\n",
    "                    ground_truth = lines[2].strip(' ')[:-1]\n",
    "                    # print(ground_truth)\n",
    "                    # print(value)\n",
    "        real_string = extract_outer_brackets(ground_truth)[0]\n",
    "        real_quoted_strings, real_remaining = extract_quoted_strings(real_string)\n",
    "        real_quoted_strings = ' '.join(real_quoted_strings)    \n",
    "  \n",
    "        try:\n",
    "            real_log_level = ground_truth.split('(')[0].split('.')[1]\n",
    "            log_level = value.split('(')[0].split('.')[1]\n",
    "        except:\n",
    "            log_level = ''\n",
    "            real_log_level = ''\n",
    "        generated_var.append(set(remaining))\n",
    "        real_var.append(set(real_remaining))\n",
    "        generated_level.append(log_level)\n",
    "        real_level.append(real_log_level)\n",
    "        quoted_strings = quoted_strings.replace('\"', '')\n",
    "        real_quoted_strings = real_quoted_strings.replace('\"', '')\n",
    "        quoted_strings = quoted_strings.lower()\n",
    "        real_quoted_strings = real_quoted_strings.lower()\n",
    "        generated_string_list.append(quoted_strings)\n",
    "        truth_string_list.append(real_quoted_strings)\n",
    "        if quoted_strings == real_quoted_strings:\n",
    "            bleu_score_list.append((1.0,1.0,1.0,1.0))\n",
    "            rouge_list.append([{\n",
    "                'rouge-1': {'f': 1.0, 'p': 1.0, 'r': 1.0},\n",
    "                'rouge-2': {'f': 1.0, 'p': 1.0, 'r': 1.0},\n",
    "                'rouge-l': {'f': 1.0, 'p': 1.0, 'r': 1.0}\n",
    "            }])\n",
    "            emb_list.append(1)\n",
    "        else:\n",
    "            bleu_score_list.append(calculate_bleu_scores(quoted_strings, real_quoted_strings))\n",
    "            rouge = Rouge()\n",
    "            scores = rouge.get_scores(quoted_strings, real_quoted_strings)\n",
    "            rouge_list.append(scores)\n",
    "            emb_list.append(unixcoder_sim(quoted_strings,real_quoted_strings))\n",
    "            #print(emb_list[-1])\n",
    "    except Exception as e:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3536"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(bleu_score_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_mean =  sum(emb_list) / len(emb_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2146"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(emb_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.5978]], device='cuda:0', grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#BLEU\n",
    "def column_averages(arr):\n",
    "    return [sum(col) / len(col) for col in zip(*arr)]\n",
    "\n",
    "print(column_averages(bleu_score_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ROUGE\n",
    "rouge1_f_scores = []\n",
    "rouge2_f_scores = []\n",
    "rougeL_f_scores = []\n",
    "for score in rouge_list:\n",
    "    scores = score[0]\n",
    "    rouge1_f_scores.append(scores['rouge-1']['f'])\n",
    "    rouge2_f_scores.append(scores['rouge-2']['f'])\n",
    "    rougeL_f_scores.append(scores['rouge-l']['f'])\n",
    "rouge1_f_score_mean = sum(rouge1_f_scores) / len(rouge1_f_scores)\n",
    "rouge2_f_score_mean = sum(rouge2_f_scores) / len(rouge2_f_scores)\n",
    "rougeL_f_score_mean = sum(rougeL_f_scores) / len(rougeL_f_scores)\n",
    "\n",
    "print(\"ROUGE-1 F-score mean:\", rouge1_f_score_mean)\n",
    "print(\"ROUGE-2 F-score mean:\", rouge2_f_score_mean)\n",
    "print(\"ROUGE-L F-score mean:\", rougeL_f_score_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#level\n",
    "print(\"AOD:\", aod(generated_level,real_level))\n",
    "print(\"Accuracy:\", level_acc(generated_level,real_level))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#var avg_precision, avg_recall, avg_f1\n",
    "\n",
    "pre = average_precision_recall_f1(real_var,generated_var)[0]\n",
    "recall = average_precision_recall_f1(real_var,generated_var)[1]\n",
    "f1 = 2*(pre*recall)/(pre+recall)\n",
    "print(pre)\n",
    "print(recall)\n",
    "print(f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
